---
title: "SR_Ch_9_HW"
author: "Samuel Snelson"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(patchwork)
library(rethinking)
library(tidybayes)
library(tidybayes.rethinking)
library(latex2exp)
library(brms)
library(rstanarm)
library(bayesplot)
library(aptheme)
theme_set(theme_light(base_family = "Avenir"))
```

## Easy Problems

### 9E1. 
Which of the following is a requirement of the simple Metropolis algorithm? \ 

1. The parameters must be discrete
Although King Markov moves between discrete spaces (i.e., discrete parameters), the simple Metropolis algorithm does not necessarily require discrete parameters. With continuous parameters, the movement proposals can be defined with respect to the k-dimensional space (where j is the number of parameters). 

\ 

2. The likelihood function must be Gaussian. 
For the simple Metropolis algorithm, the likelihood function need not be Gaussian. The purpose of the algorithm (as with all approximation methods) is to draw samples given some set of prior parameter distributions and data. The posterior can take on a wide range of shapes and is discernible by sampling from the probability density function for the posterior. We learned, however, that quadratic approximation does assume an approximately Gaussian posterior because the quad. 

\ 

3. The proposal distribution must be symmetric. 
This is a requirement of the simple Metropolis algorithm. The simple Metropolis algorithm works when proposal distributions are symmetric (the probability of any adjacent movement proposals is the same in either direction). This is not a requirement of Metropolis-Hastings nor Gibbs sampling. Proposal distribution symmetry is a concern for boundaries of parameters distributions (if a standard deviation distribution is bound at 0), and more so for providing informative proposals by which the algorithm can traverse the posterior distribution more efficiently. 

\ \ 

### 9E2. 
Gibbs sampling is more efficient than the metropolis algorithm. How does it achieve this extra efficiency? Are there any limitations to the Gibbs sampling strategy? 
\ 

Gibbs sampling is a variant of the Metropolis-Hastings algorithm. Gibbs sampling uses informative proposals which are generated by using particular combinations of likelihoods and prior distributions called conjugate priors. As discussed in the question above, Gibbs sampling is more efficient than the simple Metropolis algorithm because it makes movement proposals which are more informative (concerning the efficiency with which the algorithm discerns the shape of the posterior). Gibbs makes proposals which are more likely to be approved (given their comparative conditions).  

\ \ 

### 9E3. 

Which sort of parameters can HMC not handle? Can you explain why?  
\ 
HMC generates the posterior distribution by taking samples given by the posterior's pdf at some combination of parameter values and moving based on a fictitious momentum given by the gradient of the pdf at that location. In order for movement to be based on the gradient of that location with respect to the posterior's pdf, HMC requires continuous parameters.

\ \ 

### 9E4. 
Explain the difference between the effective number of samples, `n_eff` as calculated by Stan, and the actual number of samples. 
\ 

First, the actual number of samples refers to the number of samples the HMC algorithm has drawn (with default or user input) from the posterior pdf to generate the posterior distribution. The effective number of samples is an estimate of the number of independent samples drawn from the posterior. With the Metropolis and Gibbs approaches, adjacent proposals tend to be highly correlated with each other (i.e., autocorrelation) depending on the step size. With HMC, samples tend to be less correlated because of the momentum approach (proposals can be much further, depending on the step size and leapfrog parameters). 

\ \ 

### 9E5. 
What value should `Rhat` approach, when a chain is sampling the posterior distribution correctly. 
\ 

McElreath describes $\hat{R}$ as the ratio of the total variance across chains and the average within chain variance. $\hat{R}$ should approach 1 when a chain is sampling the posterior distribution correctly (though as a necessary but insufficient condition for an accurate posterior). 


Total variance is given by computing the variance of the means of the chains. We want chains to be exploring the same high probability space of the posterior, so between chain variance should ideally be low (with the cautionary note that multiple chains may be consistently bad and this would nonetheless generate low between chain variance). Within chain variance is given by computing the variance across samples within each chain and averaging these separate variances over the number of chains. Within chain variance will depend on the distribution of the posterior probability mass. 

\ \ 

### 9E6. 
Sketch a good trace plot for a Markov chain, one that is effectively sampling from the posterior distribution. What is good about its shape? Then sketch a trace plot for a malfunctioning Markov chain. What about its shape indicates malfunction? 

\ 

As we'll look at the rugged data shortly, we'll use it here to produce a posterior and look at the trace and trace rank plots. 

```{r}
# Cleaning data up 
data(rugged) 
d <- rugged

d <- d %>% 
  drop_na(rgdppc_2000) %>% 
  mutate(log_gdp_std = standardize(log(rgdppc_2000)), 
         rugged_c = rugged - mean(rugged),
         cid = if_else(cont_africa == 1, 1, 2)) %>% 
  select(log_gdp_std, rugged_c, cid)

```

First, we'll model the relationship between log, standardized GDP on terrain ruggedness with whether or not the nation is in Africa as a moderator. 

```{r, echo = T, results = F}
# Interaction model with sensible priors
  # log_gdp_std ~ rugged_c:cid
m1 <- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma), 
    mu <- a[cid] + bR[cid] * rugged_c,
    # with logged outcome, intercept around 1
    a[cid] ~ dnorm(1, 0.1), 
    #centered predictor, average effect of 0 
      # with variability in log_gdp_std $
    bR[cid] ~ dnorm(0, 0.3),
    sigma ~ dexp(1)
  ), data = d, chains = 4, cores = 4)
```

```{r}
precis(m1, depth = 2)
```

Neither `Rhat4` or `n_eff` give any indication that the estimation has gone astray. Out of 2,000 samples, the `n_eff` looks pretty good and `Rhat4` is all $1 \pm 0.001$. This, however, doesn't give us any indication about the model's plausibility for causal inference or predictive accuracy. These are necessary but insufficient conditions showing that the HMC algorithm worked. 

```{r}
traceplot(m1, n_cols = 2)
```

Each sampled parameter across the chains looks good on the trace plot. They each are concentrated around the largest posterior probability mass and they aren't systematically moving in one area or the other. 

We'll now look at a model of the same relationship but with some bizarre priors to see what happens to these diagnostic tools.

```{r, echo = T, results = F}
m2 <- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma), 
    mu <- a[cid] + bR[cid] * rugged_c,
    # with logged outcome, intercept around 1 
      # but with large standard deviation
    a[cid] ~ dnorm(100, 0.001), 
    #centered predictor, average effect of 0 
      # with large variability in log_gdp_std $
    bR[cid] ~ dnorm(0, 1000),
    sigma ~ dexp(0.0001)
  ), data = d, chains = 4, cores = 4)
```

```{r}
precis(m2, depth = 2)

traceplot(m2, n_cols = 2)
```

Interestingly, I have tried to reproduce something like McElreath did on pg. 295 where the chains diverged in the course of sampling. I changed the prior distributions for $\alpha, \beta_R$ and $\sigma$, but I found that the chains were concentrated. The estimation is very off because of the priors. $\alpha$ is concentrated around 100 as I have designed the prior, and it has a much smaller variance given the small standard deviation (noting the scaled y-axis for $a[1]$ and $a[2]$). The other parameters are similarly biased towards the prior distributions. 

\ \ 

### 9E7. 
Repeat the problem above, but not for a trace rank (trank) plot. 

\ \ 

First let's look at the first model with sensible priors. 

```{r, echo = T, results = F}
m1_stan <- "data{
    vector[170] log_gdp_std;
    vector[170] rugged_c;
    int cid[170];
}
parameters{
    vector[2] a;
    vector[2] bR;
    real<lower=0> sigma;
}
model{
    vector[170] mu;
    sigma ~ exponential( 1 );
    bR ~ normal( 0 , 0.3 );
    a ~ normal( 1 , 0.1 );
    for ( i in 1:170 ) {
        mu[i] = a[cid[i]] + bR[cid[i]] * rugged_c[i];
    }
    log_gdp_std ~ normal( mu , sigma );
}
"

m1_stan_fit <- stan(model_code = m1_stan, data = d)
```


```{r}
mcmc_rank_overlay(m1_stan_fit)
```

I had some weird issues producing this plot. `trankplot()` didn't like the `ulam()` model and gave an error about the length of condition. So I tried to convert the model to Stan with `stancode()` and then run the model in Stan and make the plot with `mcmc_rank_overlay()`. Initially this produced the model but the plot function didn't want to work. Then I restarted and `stancode()` didn't want to take the `ulam()` model. So I just defined the stancode as a string and plugged that into `stan()` and this worked with `mcmc_rank_overlay()`. 

But now to assessing the plot! Similar to the trace plot, each parameter is sampled across the posterior probability mass without noticeable bias. This suggests that the HMC estimation worked well given the model. 

Now let's see what the trace rank plot looks like with different priors. 

```{r, echo = T, results = F}
m2_stan <- "data{
    vector[170] log_gdp_std;
    vector[170] rugged_c;
    int cid[170];
}
parameters{
    vector[2] a;
    vector[2] bR;
    real<lower=0> sigma;
}
model{
    vector[170] mu;
    sigma ~ exponential( 0.0001 );
    bR ~ normal( 0 , 1000 );
    a ~ normal( 100 , 0.001 );
    for ( i in 1:170 ) {
        mu[i] = a[cid[i]] + bR[cid[i]] * rugged_c[i];
    }
    log_gdp_std ~ normal( mu , sigma );
}
"

m2_stan_fit <- stan(model_code = m2_stan, data = d)
```

```{r}
mcmc_rank_overlay(m2_stan_fit)
```

With respect to trace rank, the model is working but it is just wrong. In an analogous respect, the car is driving but we're going in the wrong direction. I couldn't quite choose a set of priors (though I didn't change the type of distribution) that would make the chains explore different areas of the posterior. 


## Medium Problems

### 9M1. 
Re-estimate the terrain ruggedness model from the chapter, but now using a uniform prior for the standard deviation, sigma. The uniform prior should be `dunif(0, 1)`. Use `ulam()` to estimate the posterior. Does the different prior have any detectable influence on the posterior distribution of sigma? Why or why not? 

```{r, echo = T, results = F}
m3 <- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma), 
    mu <- a[cid] + bR[cid] * rugged_c, 
    a[cid] ~ dnorm(1, 0.1), 
    bR[cid] ~ dnorm(0, 0.3), 
    sigma ~ dunif(0, 1) # uniform standard deviation [0, 1] for log_gdp_std
  ), data = d, chains = 4, cores = 4)
```

Let's see what the `precis()` outputs of the models with sigma priors of `dexp(1)` and `dunif(0, 1)` have to say: 

```{r}
precis(m1, depth = 2)
precis(m3, depth = 2)
```

Just to get a sense of what these different prior sigma distributions look like, we can visualize them as follows: 

```{r}
sd <- tibble(
  x = rexp(1e4, 1), 
  y = runif(1e4, 0, 1)
) %>% 
  pivot_longer(everything(), names_to = "var", values_to = "value")

ggplot(sd, aes(value, fill = var)) + 
  geom_density(alpha = 0.5) + 
  theme(legend.position = "none") + 
  xlim(-0.5, 4)
```

We can visualize the posterior distribution of the sigmas for each model. 

```{r}
post1 <- tidy_draws(m1, n = 1e4)

post1_sd <- post1 %>% 
  select(sigma) %>% 
  mutate(model = "Exp (1)") 

post3 <- tidy_draws(m3, n = 1e4)

post3_sd <- post3 %>% 
  select(sigma) %>% 
  mutate(model = "Unif (0, 1)")

post_sd <- rbind(post1_sd, post3_sd)


ggplot(post_sd, aes(sigma, fill = model)) + 
  geom_density(alpha = 0.5) + 
  theme(legend.title = element_blank(), 
        legend.position = "top") + 
  labs(title = "Posterior Distribution of Sigma by Model", 
        y = "Posterior Density", 
        x = "Sigma") 
```

The different priors produce quite different posterior distributions for sigma! But remember how I was playing around with the priors above and the HMC estimation was quite robust against these. Perhaps this is also the case for the estimation of the outcome. I have to work on more efficiently producing (interaction) plots with the posterior estimation methods. But I could produce two plots which account for the interaction and see how the different sigma prior effects the relationship (I suspect it wouldn't be too dramatic but perhaps not). 


\ \ 

### 9M2. 
Modify the terrain ruggedness model again. This time, change the prior for `b[cid]` to `dexp(0.3)`. What does this do to the posterior distribution?

```{r, echo = T, results = F}
m4 <- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma), 
    mu <- a[cid] + bR[cid] * rugged_c, 
    a[cid] ~ dnorm(1, 0.1), 
    bR[cid] ~ dexp(0.3), 
    sigma ~ dexp(1)
  ), data = d, chains = 4, cores = 4)

post1_b <- tidy_draws(m1, n = 1e4) %>% 
  janitor::clean_names() %>% 
  select(b_r_1:b_r_2) %>% 
  rename("Africa" = b_r_1, "Not_Africa" = b_r_2) %>% 
  mutate(model = "N (0, 0.3)")

post4_b <- tidy_draws(m4, n = 1e4) %>% 
  janitor::clean_names() %>% 
  select(b_r_1:b_r_2) %>% 
  rename("Africa" = b_r_1, "Not_Africa" = b_r_2) %>% 
  mutate(model = "Exp (0.3)")

post_b <- rbind(post1_b, post4_b)

post_b <- post_b %>% 
  pivot_longer(cols = c(Africa:Not_Africa), names_to = "term", values_to = "value")

ggplot(post_b, aes(value, fill = term)) + 
  geom_density(alpha = 0.5) + 
  facet_wrap(~ model) + 
  theme(aspect.ratio = 1.5, 
        legend.title = element_blank(), 
        legend.position = "top") + 
  labs(title = "Posterior Distribution of Slopes by Model and Continent",
       subtitle = "Slopes Represent Effect of Ruggedness on National Log, Standardized GDP",
       y = "Posterior Density", 
       x = "Slope")

```

We have an interesting effect (no pun intended). When we define the prior of the slope for ruggedness as exponentially distributed with a growth rate of 1, the posterior distribution of slopes for nations outside of Africa is much more concentrated around 0 (values between 0 and 0.25) where for African nations, their estimated effect of ruggedness on log gdp is more widely distributed around 0.75 (with a distribution which looks like a student's t). 

When we assign the prior of a normal distribution with mean 0 and standard deviation 0.3, then the distributions of effects are more similar but shifted over (from negative effects for non-African nations and positive effects for African nations). 

\ \ 

### 9M3. Re-estimate one of the Stan models from the chapter, but at differemt numbers of `warmup` iterations. Be sure to use the same number of sampling iterations in each case. Compare the `n_eff` values. How much warmup is enough? 


Just for fun, let's simulate some data where two independent predictors are related to an outcome with slope coefficients 0.5 and 2 respectively. 

```{r, echo = T, results = F}

d2 <- tibble(
  x = rnorm(1e4, 10, 2), # effect of x = 0.5
  y = rnorm(1e4, 20, 2), # effect of y = 2
  z = standardize(rnorm(1e4, 0.5 * x + 2 * y, 2))) 

warmup <- function(w) {

  m <- ulam(
    alist(
      z ~ dnorm(mu, sigma), 
      mu <- a + bX * x + bY * y, 
      a ~ dnorm(0, 1), 
      bX ~ dnorm(0, 1), 
      bY~ dnorm(0, 1), 
      sigma ~ dexp(1)),
    
    data = d2, 
    chains = 4, 
    cores = 4, 
    warmup = w, 
    iter = 1000)
  
  return(m)
  
}

m5 <- warmup(10)
m6 <- warmup(100)
m7 <- warmup(250)
m8 <- warmup(500)
m9 <- warmup(750)


```

Let’s first get a visual of the chains for each model with traceplot.


```{r}
traceplot(m5)
traceplot(m6)
traceplot(m7)
traceplot(m8)
traceplot(m9)
```

Looking at these plots sequentially, we see that the chains behave wildly until there is about 250 warmups for 1000 iterations. At 100/1000 warmups, the chains are still exploring distinct areas of the posterior distribution. However, at 250 and up, the chains stabilize on a concentrated region of the posterior.

Let’s now see if this is consistent with the number of effective samples.

```{r}
precis(m5)
precis(m6)
precis(m7)
precis(m8)
precis(m9)
```

Visually, one could plot the number of effective samples for each parameter over the warmups. Strangely, we find that the number of effective samples is highest for 250 warmups and appears to decrease, on average across parameters, the more warmups there is.

As to the simple question of how many warmups is enough, that seems to be about 250 out of 1000 iterations for this model. This is supported additionally because the parameter point estimates are essentially the same at 250 warmups and up.

\ \ 

\ \ 


